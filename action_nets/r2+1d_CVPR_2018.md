
Title:  A Closer Look at Spatialtemporal Convolutions for Action Recognition 

Authors:  Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, Manohar
Paluri

Publication Status: Published CVPR 2018

Date of Read:  25 / 09 / 18

Summary Description: This paper presents a robust empirical evaluation of
finding a mid ground between using a 2D or 3D convolutional network for action
recognition. Focusing around how temporal information is preserved and encoded
within in the network.

There are 3 main network types discussed within the network:

 - R2D - This network is essentially a stacking on all input frames within the
   first convolutional block, and then only preforming 2D convolutions
   then on afterwards.

 - MC(X) - This network is best summaries as a series of 3D convolutions
   followed by 2D convolutions. The `X' in the name is used to denote the depth
   at which convolutions are switched. `r' denoting a reverse in ordering (i.e
   (2D followed by 3D).

 - R(2+1)D - The main contribution of the paper a block consisting of  2D
   convolution, non-linearity, 1D convolution. non-linearity. The number of
   parameters used within each layer is kept equivalent to that of a 3D
   convolution.

The architectures are evaluated on kinetics and sports 1m.


Comment: This is a nice robust set of experiments, however I'm still left
interested the use of flow towards the end of the paper. Particularly the bump
in performance between RGB -> Two stream, what is leading to this combination of
these two streams, and why do the authors hypothesise that its possibly a
discrepancy in the quality of flow that account for some of the discrepancy.

What area's are captured by the flownet work that are discarded in the RGB
network, is it just the granularity with which the network is encouraged to look
at? More reading of Du Trans work estimating flow using 3D convs might be an
interesting start points. 

Additional Links: https://github.com/facebookresearch/R2Plus1D
